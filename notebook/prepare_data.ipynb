{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Dense, Flatten, Conv1D, MaxPooling1D, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import load_model\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "import joblib\n",
    "sys.path.append('../src/')\n",
    "# import vcf2onehot\n",
    "from vcf2onehot import VCF2Onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pretrain_model(data_path, label_path):\n",
    "    print('preparing data for pretrain model...')\n",
    "        \n",
    "    batch_data = sorted(glob.glob(data_path))\n",
    "    batch_label = sorted(glob.glob(label_path))\n",
    "    \n",
    "    process_data = VCF2Onehot(data_path=batch_data, label_path=batch_label)\n",
    "    \n",
    "    n_batch = 12\n",
    "    n_batch_train = 10\n",
    "    \n",
    "    print(f'create {n_batch} batch: {n_batch_train} batch train and {n_batch - n_batch_train} batch test')\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        activate_score = []\n",
    "        # Generate the seq data\n",
    "        vcf = batch_data[i]\n",
    "        # print(vcf)\n",
    "        seqs = process_data.build_seqs(vcf=vcf)\n",
    "        # Create the data object\n",
    "        data = process_data.format_seqs(seqs)\n",
    "\n",
    "        with open(f'{batch_label[i]}') as f:\n",
    "            for line in f:\n",
    "                activate_score.append(line.split(',')[-2])\n",
    "        \n",
    "        data[\"activate_score\"] = np.array(activate_score, dtype=np.float64)\n",
    "        \n",
    "        file_name = vcf.split('\\\\')[-1].split('.')[0]\n",
    "        \n",
    "        if i < n_batch_train:\n",
    "            joblib.dump(data, f'../data/pretrained_model/train/{file_name}.joblib')\n",
    "            print(f\"save train batch {i + 1}...\")\n",
    "        else:\n",
    "            joblib.dump(data, f'../data/pretrained_model/test/{file_name}.joblib')\n",
    "            print(f'save test batch {(i - n_batch_train) + 1}...')\n",
    "        \n",
    "    print(\"Done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_names(star_sample) -> list:\n",
    "    samples = []\n",
    "    with open(star_sample) as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#CHROM\"):\n",
    "                samples = line.strip().split()[9:]\n",
    "                break\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label_data_transfer(star_sample, curated_func, save_label):\n",
    "    print('processing label for transfer learning model...')\n",
    "        \n",
    "    alleles_function = pd.read_excel(curated_func, usecols=[0, 1])\n",
    "    \n",
    "    samples = get_sample_names(star_sample)\n",
    "        \n",
    "    labels = []\n",
    "    for sample in samples:\n",
    "        star = \"*\" + str(sample.split(\"_\")[1])\n",
    "        label = alleles_function[alleles_function[\"CYP2D6 Star Allele\"] == star][\"Curated Function\"].values[0]\n",
    "\n",
    "        if label == \"Uncurated\":\n",
    "            no_function = None\n",
    "            normal_function = None\n",
    "        else:\n",
    "            no_function = 0 if label == \"No function\" else 1\n",
    "            normal_function = 1 if label == \"Normal\" else 0\n",
    "\n",
    "        labels.append([sample, no_function, normal_function])\n",
    "        \n",
    "    label_df = pd.DataFrame(labels)\n",
    "    label_df.to_csv(save_label, index=False, header=None)\n",
    "    print(\"Saved labels to %s\" % save_label)\n",
    "    print('Done!\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path, label_path):\n",
    "    samples = get_sample_names(data_path)\n",
    "    process_data = VCF2Onehot(data_path=data_path, label_path=label_path)\n",
    "    \n",
    "    seqs = process_data.build_seqs(data_path)\n",
    "    data = process_data.format_seqs(seqs)\n",
    "    data['y'] = pd.read_csv(label_path, header=None, index_col=0).loc[samples].values\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data: dict) -> tuple:\n",
    "    determined_samples = {} # lưu những mẫu đã xác định chức năng\n",
    "    uncurated_samples ={} # lưu những mẫu chức năng chưa xác định chức năng\n",
    "    \n",
    "    mask = np.all(np.isnan(data['y']) == False, axis=1)\n",
    "    \n",
    "    determined_samples['X'] = data['X'][mask]\n",
    "    determined_samples['y'] = data['y'][mask]\n",
    "    determined_samples['sample_names'] = data['sample_names'][mask]\n",
    "\n",
    "    uncurated_samples['X'] = data['X'][~mask]\n",
    "    uncurated_samples['sample_names'] = data['sample_names'][~mask]\n",
    "\n",
    "    uncurated_stars = np.array([s for s in uncurated_samples['sample_names'] if s.split('_')[-1] == '001']) # chỉ lấy những mẫu cs subalen = 001 để dự đoán\n",
    "    uncurated_star_mask = np.isin(uncurated_samples['sample_names'], uncurated_stars) \n",
    "    \n",
    "    uncurated_samples['sample_names'] = uncurated_samples['sample_names'][uncurated_star_mask]\n",
    "    uncurated_samples['X'] = uncurated_samples['X'][uncurated_star_mask]\n",
    "    \n",
    "    return determined_samples, uncurated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transfer_model(data_path, label_path):\n",
    "    print('preparing data for transfer model...')\n",
    "    data = get_data(data_path, label_path)\n",
    "    \n",
    "    determined_samples, uncurated_samples = split_data(data)\n",
    "    joblib.dump(uncurated_samples, '../data/final_model/uncerated/uncerated.joblib')\n",
    "    print(f'saved uncerated samples to ../data/final_model/uncerated/uncerated.joblib')\n",
    "    \n",
    "    \n",
    "    all_stars = np.array([s.split('_')[1] for s in determined_samples['sample_names']]) # lấy ra star alen: 10, 1, 2, ...\n",
    "    stars, idx = np.unique(all_stars, return_index=True)\n",
    "    train_idx, test_idx = train_test_split(idx, stratify=determined_samples['y'][idx], test_size=24, random_state=10)\n",
    "    \n",
    "    sample_mask = np.isin(all_stars, all_stars[train_idx]) # Đánh dấu những star allele của train_idx trong all_stars\n",
    "\n",
    "    # gồm tất cả các allele + suballele sử dụng trong quá trình trainning\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    \n",
    "    train_data['X'], train_data['y'], train_data['sample_names']= determined_samples['X'][sample_mask], determined_samples['y'][sample_mask], determined_samples['sample_names'][sample_mask]\n",
    "    test_data['X'], test_data['y'], test_data['sample_names'] = determined_samples['X'][~sample_mask], determined_samples['y'][~sample_mask], determined_samples['sample_names'][~sample_mask]\n",
    "    print(train_data)\n",
    "    joblib.dump(train_data, f'../data/final_model/train/train_data.joblib')\n",
    "    print(f\"saved train data to ../data/final_model/train/train_data.joblib\")\n",
    "    \n",
    "    joblib.dump(test_data, f'../data/final_model/test/test_data.joblib')\n",
    "    print(f\"saved test data to ../data/final_model/test/test_data.joblib\")\n",
    "    print('Done!\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    DATA_PATH1 = '../data/simulated_cyp2d6_diplotypes/*.vcf'\n",
    "    LABEL_PATH1 = '../data/simulated_cyp2d6_diplotypes/*.csv'\n",
    "    \n",
    "    DATA_PATH2 = '../data/final_model/star_samples.vcf'\n",
    "    LABEL_PATH2 = '../data/final_model/labels_alleles.csv'\n",
    "    CERATED_FUNC = '../data/final_model/pcbi.1008399.s003.xlsx'\n",
    "    # prepare_pretrain_model(data_path=data_path1, label_path=label_path1)\n",
    "\n",
    "    # process_label_data_transfer(star_sample=DATA_PATH2, curated_func=CURATED_FUNC, save_label=LABEL_PATH2)\n",
    "    \n",
    "    prepare_transfer_model(data_path=DATA_PATH2, label_path=LABEL_PATH2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved uncurated samples to ../data/final_model/uncurated/uncerated.joblib\n",
      "{'X': array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 1., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 0.],\n",
      "        [0., 0., 0., ..., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 1., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 0.],\n",
      "        [0., 0., 0., ..., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 1., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 0.],\n",
      "        [0., 0., 0., ..., 0., 1., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 1., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 0.],\n",
      "        [0., 0., 0., ..., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 1., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 0.],\n",
      "        [0., 0., 0., ..., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0., ..., 0., 1., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 0.],\n",
      "        [0., 0., 0., ..., 0., 1., 0.]]]), 'y': array([[1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [0., 0.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [0., 0.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [1., 1.],\n",
      "       [1., 1.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [1., 0.],\n",
      "       [1., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [1., 0.]]), 'sample_names': array(['CYP2D6_10_001', 'CYP2D6_10_002', 'CYP2D6_10_003', 'CYP2D6_10_004',\n",
      "       'CYP2D6_1_001', 'CYP2D6_1_002', 'CYP2D6_1_003', 'CYP2D6_1_004',\n",
      "       'CYP2D6_1_005', 'CYP2D6_1_006', 'CYP2D6_1_007', 'CYP2D6_1_008',\n",
      "       'CYP2D6_1_009', 'CYP2D6_101_001', 'CYP2D6_1_010', 'CYP2D6_1_011',\n",
      "       'CYP2D6_1_012', 'CYP2D6_1_013', 'CYP2D6_1_014', 'CYP2D6_1_015',\n",
      "       'CYP2D6_1_016', 'CYP2D6_1_017', 'CYP2D6_1_018', 'CYP2D6_1_019',\n",
      "       'CYP2D6_101_vcf', 'CYP2D6_1_020', 'CYP2D6_1_021', 'CYP2D6_1_022',\n",
      "       'CYP2D6_1_023', 'CYP2D6_1_024', 'CYP2D6_1_025', 'CYP2D6_1_026',\n",
      "       'CYP2D6_1_027', 'CYP2D6_1_028', 'CYP2D6_1_029', 'CYP2D6_1_030',\n",
      "       'CYP2D6_1_031', 'CYP2D6_1_032', 'CYP2D6_10_vcf', 'CYP2D6_11_001',\n",
      "       'CYP2D6_114_001', 'CYP2D6_114_vcf', 'CYP2D6_11_vcf',\n",
      "       'CYP2D6_15_001', 'CYP2D6_15_002', 'CYP2D6_15_003', 'CYP2D6_15_vcf',\n",
      "       'CYP2D6_17_001', 'CYP2D6_17_002', 'CYP2D6_17_003', 'CYP2D6_17_vcf',\n",
      "       'CYP2D6_18_001', 'CYP2D6_18_vcf', 'CYP2D6_19_001', 'CYP2D6_19_vcf',\n",
      "       'CYP2D6_2_001', 'CYP2D6_2_002', 'CYP2D6_2_003', 'CYP2D6_2_004',\n",
      "       'CYP2D6_2_005', 'CYP2D6_2_006', 'CYP2D6_2_007', 'CYP2D6_2_008',\n",
      "       'CYP2D6_2_009', 'CYP2D6_2_010', 'CYP2D6_2_011', 'CYP2D6_2_012',\n",
      "       'CYP2D6_2_013', 'CYP2D6_2_014', 'CYP2D6_2_015', 'CYP2D6_2_016',\n",
      "       'CYP2D6_2_017', 'CYP2D6_2_018', 'CYP2D6_2_019', 'CYP2D6_2_020',\n",
      "       'CYP2D6_27_001', 'CYP2D6_27_vcf', 'CYP2D6_31_001', 'CYP2D6_31_vcf',\n",
      "       'CYP2D6_35_001', 'CYP2D6_35_002', 'CYP2D6_35_003', 'CYP2D6_35_004',\n",
      "       'CYP2D6_35_005', 'CYP2D6_35_006', 'CYP2D6_35_007', 'CYP2D6_35_vcf',\n",
      "       'CYP2D6_36_001', 'CYP2D6_36_002', 'CYP2D6_36_vcf', 'CYP2D6_38_001',\n",
      "       'CYP2D6_38_vcf', 'CYP2D6_4_001', 'CYP2D6_4_002', 'CYP2D6_4_003',\n",
      "       'CYP2D6_4_004', 'CYP2D6_4_005', 'CYP2D6_4_006', 'CYP2D6_4_007',\n",
      "       'CYP2D6_4_008', 'CYP2D6_4_009', 'CYP2D6_4_010', 'CYP2D6_4_011',\n",
      "       'CYP2D6_4_012', 'CYP2D6_4_013', 'CYP2D6_4_014', 'CYP2D6_4_015',\n",
      "       'CYP2D6_4_016', 'CYP2D6_4_017', 'CYP2D6_4_018', 'CYP2D6_4_019',\n",
      "       'CYP2D6_4_020', 'CYP2D6_4_021', 'CYP2D6_4_022', 'CYP2D6_4_023',\n",
      "       'CYP2D6_4_024', 'CYP2D6_4_025', 'CYP2D6_4_026', 'CYP2D6_4_027',\n",
      "       'CYP2D6_4_028', 'CYP2D6_41_001', 'CYP2D6_41_002', 'CYP2D6_41_003',\n",
      "       'CYP2D6_41_004', 'CYP2D6_41_005', 'CYP2D6_41_vcf', 'CYP2D6_47_001',\n",
      "       'CYP2D6_47_vcf', 'CYP2D6_48_001', 'CYP2D6_48_vcf', 'CYP2D6_49_001',\n",
      "       'CYP2D6_49_vcf', 'CYP2D6_4_vcf', 'CYP2D6_51_001', 'CYP2D6_51_vcf',\n",
      "       'CYP2D6_53_001', 'CYP2D6_53_vcf', 'CYP2D6_55_001', 'CYP2D6_55_vcf',\n",
      "       'CYP2D6_56_001', 'CYP2D6_56_002', 'CYP2D6_56_003', 'CYP2D6_56_vcf',\n",
      "       'CYP2D6_57_001', 'CYP2D6_57_vcf', 'CYP2D6_60_001', 'CYP2D6_60_vcf',\n",
      "       'CYP2D6_62_001', 'CYP2D6_62_vcf', 'CYP2D6_72_001', 'CYP2D6_72_vcf',\n",
      "       'CYP2D6_8_001', 'CYP2D6_8_vcf', 'CYP2D6_9_001', 'CYP2D6_9_002',\n",
      "       'CYP2D6_92_001', 'CYP2D6_92_vcf', 'CYP2D6_9_vcf'], dtype='<U14')}\n",
      "saved test data to ../data/final_model/train/train_data.joblib\n",
      "saved test data to ../data/final_model/test/test_data.joblib\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-hus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
